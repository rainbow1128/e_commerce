# 
input { 
    tcp {
        port => 5959
        codec => "line"
        ssl_enable => false
    }
} # end of input phase


filter {
    fingerprint {
        source => "message"
        target => "[@metadata][fingerprint]"
        # for C10K problem, maximum number of the log messages per day are
        # 3600 * 24 * 10000 * k, where k is number of log messages per frontend request
        # ,if k = 100, the number above is around 2^(36.3303...), so generated value for
        # the fingerprints unlikely collide (in just one day)
        method => "SHA1"
        key => "xxx"
    }

    grok {
        break_on_match => true
        named_captures_only => true
        ##patterns_dir => ['/etc/logstash/custom_pattern']
        match => {
            "message" => [
                ## comment, for development use
                ## URI parameters, process ID, and thread ID are optional
                ## ISO8601 timestamp format --> YYYY-MM-DD hh:mm:ss,uuu
                "(%{IP:[request][ip]} %{WORD:[request][mthd]} %{URIPATH:[request][uri][path]}(?:%{URIPARAM:[request][uri][params]})? )?%{TIMESTAMP_ISO8601:[@metadata][asctime]} %{LOGLEVEL:level} %{INT:process} %{INT:thread} %{PATH:[code][filepath]} %{INT:[code][lineno]} %{GREEDYDATA:serial_json_msg}",
                ## comment, for logging user activity
                "%{IP:[request][ip]} %{WORD:[reqeust][mthd]} %{URIPATH:[request][uri][path]} %{INT:[profile][id]} %{WORD:[profile][firstname]} %{WORD:[affected][model_cls]}"
            ]
        }
    } ## end of grok plugin

    grok {
        # extract year and month from input event time (the app server)
        # for internal use, all nested fields in @metadata field are not
        # part of event at output time.
        match => {
            "[@metadata][asctime]" => [
                "%{YEAR:[@metadata][evt_date][year]}-%{MONTHNUM:[@metadata][evt_date][month]}-%{GREEDYDATA:[@metadata][evt_date][useless]}"
            ]
        }
    }

    mutate {
        remove_field => ["[@metadata][evt_date][useless]"]
    }

    date {
        match => ["[@metadata][asctime]", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS"]
        timezone => "Asia/Taipei"
        target => "@timestamp"
        ##remove_field => []
    }

    ## filters can be chained like this
    if [serial_json_msg] {
        #kv 
        json {
            source => "serial_json_msg"
            target => "msg_kv_pairs"
            #value_split => "="
            #field_split => "&"
        }
        if "_jsonparsefailure" not in [tags] and [msg_kv_pairs] {
            mutate {
                remove_field => ["serial_json_msg"]
            }
        }
    }

    if "_grokparsefailure" not in [tags] {
        mutate {
            remove_field => ["message"]
        }
    } ## keep unparsed message & ship to output phase
} ## end of filter phase


output {
    elasticsearch {
        hosts => ["localhost:9200"]
        user => "CHANGE_YOUR_USERNAME"
        password => "CHANGE_YOUR_PASSWORD"
        action => "index"
        # log indexes are separate by months
        # NOTE: DO NOT read event time by using syntax %{+yyyy.MM.dd.HH.mm}
        # because I have not figured out how to change the timezone
        # (it might be impossible to change that) 
        #### index => "log-%{+yyyy.MM.dd.HH.mm}"
        index => "log-%{[@metadata][evt_date][year]}-%{[@metadata][evt_date][month]}"
        document_type => "app_server" # deprecated in v 7.x
        document_id   => "%{+dd}%{[@metadata][fingerprint]}"
    }
    ##stdout { codec => rubydebug }
} # end of output phase

